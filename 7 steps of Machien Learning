https://www.youtube.com/watch?v=nKW8Ndu7Mjw

The 7 steps of Machine Learning
1. Gathering Data
2. Preparing that Data
3. Choosing a Model
4. Training
5. Evaludation
6. Hyperparameter Tuning
7. Prediction

1. Gathering Data: the data you gathered will directly determine how good your prediction model can be -> Training Data

2. Preparing Data: First, we'll first put all data together then randomized the ordering. This is also a good time to do any pertinent visualization. 
                   This will help us find any relevant relationships between different variables as well as show you if there are any data imbalances.
                   Splitting the data to Training data and Evaluation.
                   Sometimes the data we collected needs other forms of adjusting and manipulation -  things likes duplication, normalization, error
                   correction, and others.

3. Choosing a Model

4. Training: In this step, we will use our data to incrementally improve our model's prediction ability .
             The formular for a straight line is y=m*x+b, where x is input, m is the slope of the line, b is the y-intercept and y is the value of the
             line at that position x.
             The values we have available to use to adjust or train are just m and b, where is the m is the slope and n is the intercept. There is no 
             othe way to affect the position of the line.
             In machine learning, there are many ms, since there maybe many features. The collection of these values is usually formed into a matrix 
             that is denoted w for the weight matix. Similary, for b, we arranged them together, and that's called the biases. 
             Weights= [ m_1,1 m_1,2; m_2,1 m_2,2;m_3,1 m_3,2] 
             Biases - [b_1,1 b_1,2; b_2,1 b_2,2; b_3,1 b_3,2]             
             The training process involves initialzing some random values for w and b and attempting to predict the outputs with those values.
             As you can imagine, it does pretty poorly at first. But we can compare our model's predictions with the output that it should have
             produced and adjust the values in w and b such that we will have more accurate predictions on the next time aroung.
             So this process then repeats. Each iteration or cycle of updating the weights and biases is called one training step.

5. Evaluation: Once the training step is complete, it's time to see if the model is any good. Using evaluation, this is where that databaset that we set
               aside earlier comes into play. Evaluation allows us to test our model against data that has never been used for training.
               This metric allows us to see how the model might perform against data that it has not yet seen. This is meant to be representative of how
               the model might perform in the real world.
               A good rule of thumb for a training-evaluation split is somewhere on the order of 80% 20% or 70% 30%. Much of this depends on the size of
               the original source dataset. If you have a lot of data, you don't need a big fraction for the evaluation dataset.

6. Hyperparameter Tunning: Once you have done evaluation, it's possible that you want to see if you can further improve your training in any way.
                           We can do this by tunning some of our parameters. There are a few that we implicitly assumed when we are did our training,
                           and now is a good time to go back and test those assumptions, try othe values.
                           One example of a parameter we can tune is how many time we run through the training set during training. We can actually show
                           the data multiple times. So, by doing that, we will potentially lead to higher accuracies.
                           Another paramter is learning rate. This defines how we shift the line during each step based on the information from the previous
                           training step.
                           These values all play a role in how accurate our model can become and how long that training takes.
                           These parameters are typically referred to hyperparameters. The adjustment or tuning of these hyperparameters still remains 
                           a bit more of an art than a science and it's an experimental process that heavily depends on the specifies of your dataset,
                           model, and training processes.

7. Prediction: Once you are happy with your evaludations and hyperparamters, guided by the evaludation step, it's finally time to use your model to do
               something useful. Machine Learning is using data to answer questions, so prediction or inference is that step where we finally get to
               answer some questions. This is the point of all of this work where the value of machine learning is realized.
               
               
The Power of Machine Learning is that we are able to determine  xxx through our model rather than using huam judgements and manual rules.         
              
            
         

