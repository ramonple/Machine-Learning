------------------------------- Course 1: Exploratory Data Analysis for Machine Learning --------------------------------------------------------------------------------------------------------

----- Data Cleaning

Messy data?
duplicate or uncessary data
inconsistent text and tyos
missing data
outliers
data sourcing issues: multiple systems, different database types, ...

Missing data:
Remove the data: remove the row(s) entirely
Impute the data: replace with substitued values. Fill in the missing data with the most common value, the average value, etc.
Mask the data: create a catergory for missing values

Outliers
( An outlier is an observation in data that is distant from most other observations.)
It is important to remember that some outliers are informative and provide insights into the data.

How  to find outliers?
Plots: Histogram, Density Plot, Box plot
Statistics: standard deviation, interquartile range (iqr = q75-q25), min/ max limits to be considered an outlier min =q25-1.5*(iqr); max=q75+1.5*(iqr)
Residuals: standardized

## sns.displot(data,bins=20);  -- histogram
## sns.boxplot(data); -- boxplot

EDA Exploratory Data Analysis
EDA is going to be the approach to analyzing data sets to summarize their main characteristics, often with visual methods, and as we'll see what's statistical summary is as well. 
visualizations: histograms, scatter plot, boxplot,...


Data Transformations:
a. Log transformations. y= b0 + b1 log(x)
b. Polynomial transformation y= b0 + b1x + b2x^2 + b3x^3
Variable Selection: involves choosing the set of features in include in the model. Variables must often be transformed, as mentioned before, before they can even 
be included in our models. In addition to a log and polynomial transformation, this can involve:
c. Encoding: converting non-numeric features such as categorical or ordinal features to numeric features
d. Scaling: which will be just converting the scale of numeric data so that they're on a comparable scale.

Encoding is often applied to 'categorical features' (non-numeric): nomial (red,green,blue), ordinal (high,medium,low)
- Binary encoding : 0 or fail, 1 for success
- one-hot encoding: converts variables tha take multiple values into binary(0,1) variables. One for each category.
- Ordinary encoding: convery catogories into numbers, 0,1,2,3,4

Feature Scalling involves adjusting a variables scale. Different continuous features often have differnt scales.
- standard scaling: converts features to standard normal variables
- min-max scaling: convert variables to continous variables in the (0,1) interval by mapping a minimum values to0 and maximum to 1.
                   This type of scaling is sensitive to outliers.
- Robust scaling: is similar to min-max scaling, but instead maps the interquartile range to (0,1).

Common variable transformation:
Feature Type                                      Transformation
Continuous: numerical values                      - standard, min-max scaling, robust scaling
- Nominal: Categorical, unordered features        - binary, one-hot encoding
- Ordinal: categorical, ordered featuers          - ordinal encoding (0,1,2,3,4)


Estimation and Inference

Estimation is the application of an algorithm, e.g., taking an averange.
Inference: involves putting an accuracy on the estimated (std).

parametric vs. non-parametric
      
Frequentist vs. Bayesian Statistics

A frequentiest is concerned with repeated observations in the limit.
Approach:
1. derive the probabilstic property of a procedure
2. apply the probability directly to the obsevered data

Bayesian describes parameters by probability distribution. Before seeing any data, a prioir distirbution is formulated.
The prioir distribution is then updated aftering seeing the data. After updating, the distribution is called the posterioir distribution.

The element that differs in the interpretation.


The p-value is just the smallest significance level at which the null hypothesis would be rejected. 
But once you have chosen a significance level, e.g. 0.05, it is incorrect to interpret p-values in reference to how close they are to your significance level.

Hypothesis Testing
A hypothesis is a statemnet about a population parameter.

likelihood ratio is called a test statistic, we use it to decide whether to reject H0 or not.


Correlation vs. Causation
A correlation between variables, however, does not automatically mean that the change in one variable is the cause of the change in the values of the 
other variable. Causation indicates that one event is the result of the occurrence of the other event; i.e. there is a causal relationship between the 
two events.
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------






-----------------------------------  Course 2 : Suprevised Machine Learning: Regression --------------------------------------------------------------------------------------------------------
examples of models for supervised learing: regression
1 feature => linear regresion, non-linear regression
multiple features => linear regresion, non-linear regression

Supervised Machine Learning for Interpretation and Prediction

interpretation can provide insight into improvements in prediction, and vice-versa. (But not all)

Two types of supervised maching learning: Regression VS Classification


-----------    Regression: outcome is continuous (numerical)
data with outcomes + model  -----fit----> model
data without outcomes + model ---predict---> predicted outcomes


-----------  Classification: Categorical Answer
Labeled data + model   -----fit----> model
Unlabeled data + model ---predict---> predicted outcomes


----------- Linear Regression
y=b0+b1x
residuals: predicted value - observed value

minimizing the error function:
J(b0,b1) =  min < 1/2m * SUM_(from i=1 to m) ( (b0 + b1 * x1_{observed}) - Y_{observed} ) ^ 2> 
----   The 1𝑚 is to "average" the squared error over the number of components so that the number of components doesn't affect the function (see John's answer). 
-----  So now the question is why there is an extra 1/2. In short, it doesn't matter. The solution that minimizes 𝐽 as you have written it will also minimize 2𝐽=1𝑚∑𝑖(ℎ(𝑥𝑖)−𝑦𝑖)2. The latter function, 2𝐽, may seem more "natural," but the factor of 2 does not matter when optimizing.
------ The only reason some authors like to include it is because when you take the derivative with respect to 𝑥, the 2 goes away.

Sum of Squared Error (SSE):  sum_(from i to m) (y_{b} (x^1) - y^i_{obs}) ^ 2
Total Sum of Squre (TSS):  sum_(from i to m) ( [y_{obs}] bar - y^i_{obs}) ^ 2
Coefficient of Determination:(R^2) = 1 - SSE/TSS =  (TSS − RSS)/TSS
          --   the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.


---------- Training and Test splits


---------- Cross Validation : evaluating estimator performance

training data 1
test data 1

re-select 
training data 2
validation 2

re-select
training data 3
validation 3

There is no overlap in each signle run.

complexity vs. Error
In machine learning, model complexity often refers to the number of features or terms included in a given predictive model.

overfitting: training error is low, cross validation error is high --> the variance is too high
JUST RIGHT: both training and cross validation error are low
underfitting: both training and cross validation error are high

Cross validation approaches:
k-fold cross validation: Using each of k subsamples as a test sample
leave one out cross validation： Using each observation as a test sample
stratified cross validation: k-fold cross validation with representative samples


----------  Polynomial Regression

y=b0+b1x+b2x^2+b3x^3

can also include variable interactions: y=b0+b1x+b2x2+b3x1x2



------------------------------------ Regularization Techniques

------ Bias Variance TradeOff
bias: is a tendency to miss
Variance: is a tendency to be inconsistent

Bias: being wrong
Vairance: being unstanble
Irreducible error: unavoidable randomness

------ Regularization and Model Selection 
In simple terms, regularization is tuning or selecting the preferred level of model complexity so your models are better at predicting (generalizing). 

How does Regularization acoomplished?
Regularization adds an (adjustable) regularization strength parameter directly into the cost function.

M(w) = lambda * R(w).   <-- Adjusted cost function 

M(w) model error
R(w) function of estimated parameters
lambda: regularization strengh parameter
This lambda adds a penalty proportional to the size of the estimated model parameter, or a function of the parameter.

The regularization strength parameter lambda allows us to manage the complexity tradeoff:
-- more regularization introduces a simpler model or more bias (large lambda)
-- less regularization introduction makes the model more complex and increase variance (small lambda)
If our model is overfit (variance is too high), regularization can improve the generalization error and reduce variance.

Regularization performs feature selections by shrinking the contribution of features.
L1 regularization, this is accomplished by driving some coeifficients to zero.
Feature selection can also be realized through reducing features.

Reducing the number of features can prevent overfitting (high variance).

------ Ridge Regression
Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where independent 
variables are highly correlated. 


------ LASSO Regression
Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, 
like the mean


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


----------------------------------------------------  Course 3 Suprvised Machine Learning: Classification --------------------------------------------------------------------------------------------------------
examples of models used for supervised learning: classification
Logistic regression
k-nearest neighbors
support vector machines
neural network
decision tree
random forest
boosting
ensemble models

Each of these models can be used for both regression and classifcation.



------------------------------------------------------------   Logistic Regression -----------

Curve: S-shape
logistic function: y = 1/ ( 1 + exp(- (-b0+b1x+eta) )    )
odds ratio: log(p(x) / 1 - p(x) ) = b0+b1x

---------------- Confusion Matrix, accuracy, specificity, precision and recall 

confusion matrix ( the one shows the type I type II errors)
                           Predicted Positive       Prediticed Negative
Actual positive                    TP                   FN (Type II error)  --> false negative

Actual Negative                FP (Type I error)         TN
                              false postitive
                              
                      (True Positive Rate)  Recall or sensitivity = TP/ (TP + FN). --> correctly find the postive one when it is really positive

                              Precision = TP/ (TP+FP). -->  correctly find the postive one when it is predicted postive 
                              
 ( False Positive Rate = 1- Specificity)    Specificity = TN/ (FP+TN)
                              
Accuracy = (TP+TN)/(TP+FN+TF+NF)

F1 = 2 * Precision * Recall / ( Recision + Recall)


----------------  Classification Error Metrics: ROC and Precision-Recall Curves

receiver operating characteristic (ROC) 
The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). 
Classifiers that give curves closer to the top-left corner indicate a better performance.


Precision - Recall Curve: measures trad-off betwee precision and recall

ROC curve: better for data with balanced classes
Precision-Recall curve: generally better for data with imbalanced classes


multi-class error metircs: similar to binary versions, just expand elements as a sum




---------------------------------------------- K Nearest Neighbors (KNN) for Classification ------------

What is needed to KNN?
a. correct value for K
b. How to measure closeness of neighborrs

Distance Measurement: 
Euclidean Distance d= sqrt( (x1-x2)^2 + (y1-y2)^2)
Manhattan Distance d= |x1-x2|  + |y1-y2|

Scale for distance measurement:
feature scaling




！！！ -------------------------------------------- Support Vector Machines  SVM----------------
In the SVM algorithm, we plot each data item as a point in n-dimensional space, with the value of each feature being the value of a particular coordinate. 
Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.
Maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane.

We have a <decision boundary /decision surface > b1x1+b2x2+b0 = 0 
we should also plot two lines which are parellel to this decision boundary, and 

Linear classifier
Object: find the best beta to describe the margin boundary (hyper-plane)
dis = 2/||beta|| --> Maximize 
  => min ||Beta||^2 /2



---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
