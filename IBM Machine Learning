------------------------------- Course 1: Exploratory Data Analysis for Machine Learning --------------------------------------------------------------------------------------------------------

----- Data Cleaning

Messy data?
duplicate or uncessary data
inconsistent text and tyos
missing data
outliers
data sourcing issues: multiple systems, different database types, ...

Missing data:
Remove the data: remove the row(s) entirely
Impute the data: replace with substitued values. Fill in the missing data with the most common value, the average value, etc.
Mask the data: create a catergory for missing values

Outliers
( An outlier is an observation in data that is distant from most other observations.)
It is important to remember that some outliers are informative and provide insights into the data.

How  to find outliers?
Plots: Histogram, Density Plot, Box plot
Statistics: standard deviation, interquartile range (iqr = q75-q25), min/ max limits to be considered an outlier min =q25-1.5*(iqr); max=q75+1.5*(iqr)
Residuals: standardized

## sns.displot(data,bins=20);  -- histogram
## sns.boxplot(data); -- boxplot

EDA Exploratory Data Analysis
EDA is going to be the approach to analyzing data sets to summarize their main characteristics, often with visual methods, and as we'll see what's statistical summary is as well. 
visualizations: histograms, scatter plot, boxplot,...


Data Transformations:
a. Log transformations. y= b0 + b1 log(x)
b. Polynomial transformation y= b0 + b1x + b2x^2 + b3x^3
Variable Selection: involves choosing the set of features in include in the model. Variables must often be transformed, as mentioned before, before they can even 
be included in our models. In addition to a log and polynomial transformation, this can involve:
c. Encoding: converting non-numeric features such as categorical or ordinal features to numeric features
d. Scaling: which will be just converting the scale of numeric data so that they're on a comparable scale.

Encoding is often applied to 'categorical features' (non-numeric): nomial (red,green,blue), ordinal (high,medium,low)
- Binary encoding : 0 or fail, 1 for success
- one-hot encoding: converts variables tha take multiple values into binary(0,1) variables. One for each category.
- Ordinary encoding: convery catogories into numbers, 0,1,2,3,4

Feature Scalling involves adjusting a variables scale. Different continuous features often have differnt scales.
- standard scaling: converts features to standard normal variables
- min-max scaling: convert variables to continous variables in the (0,1) interval by mapping a minimum values to0 and maximum to 1.
                   This type of scaling is sensitive to outliers.
- Robust scaling: is similar to min-max scaling, but instead maps the interquartile range to (0,1).

Common variable transformation:
Feature Type                                      Transformation
Continuous: numerical values                      - standard, min-max scaling, robust scaling
- Nominal: Categorical, unordered features        - binary, one-hot encoding
- Ordinal: categorical, ordered featuers          - ordinal encoding (0,1,2,3,4)


Estimation and Inference

Estimation is the application of an algorithm, e.g., taking an averange.
Inference: involves putting an accuracy on the estimated (std).

parametric vs. non-parametric
      
Frequentist vs. Bayesian Statistics

A frequentiest is concerned with repeated observations in the limit.
Approach:
1. derive the probabilstic property of a procedure
2. apply the probability directly to the obsevered data

Bayesian describes parameters by probability distribution. Before seeing any data, a prioir distirbution is formulated.
The prioir distribution is then updated aftering seeing the data. After updating, the distribution is called the posterioir distribution.

The element that differs in the interpretation.


The p-value is just the smallest significance level at which the null hypothesis would be rejected. 
But once you have chosen a significance level, e.g. 0.05, it is incorrect to interpret p-values in reference to how close they are to your significance level.

Hypothesis Testing
A hypothesis is a statemnet about a population parameter.

likelihood ratio is called a test statistic, we use it to decide whether to reject H0 or not.


Correlation vs. Causation
A correlation between variables, however, does not automatically mean that the change in one variable is the cause of the change in the values of the 
other variable. Causation indicates that one event is the result of the occurrence of the other event; i.e. there is a causal relationship between the 
two events.
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------






-----------------------------------  Course 2 : Suprevised Machine Learning: Regression --------------------------------------------------------------------------------------------------------
examples of models for supervised learing: regression
1 feature => linear regresion, non-linear regression
multiple features => linear regresion, non-linear regression

Supervised Machine Learning for Interpretation and Prediction

interpretation can provide insight into improvements in prediction, and vice-versa. (But not all)

Two types of supervised maching learning: Regression VS Classification


-----------    Regression: outcome is continuous (numerical)
data with outcomes + model  -----fit----> model
data without outcomes + model ---predict---> predicted outcomes


-----------  Classification: Categorical Answer
Labeled data + model   -----fit----> model
Unlabeled data + model ---predict---> predicted outcomes


----------- Linear Regression
y=b0+b1x
residuals: predicted value - observed value

minimizing the error function:
J(b0,b1) =  min < 1/2m * SUM_(from i=1 to m) ( (b0 + b1 * x1_{observed}) - Y_{observed} ) ^ 2> 
----   The 1𝑚 is to "average" the squared error over the number of components so that the number of components doesn't affect the function (see John's answer). 
-----  So now the question is why there is an extra 1/2. In short, it doesn't matter. The solution that minimizes 𝐽 as you have written it will also minimize 2𝐽=1𝑚∑𝑖(ℎ(𝑥𝑖)−𝑦𝑖)2. The latter function, 2𝐽, may seem more "natural," but the factor of 2 does not matter when optimizing.
------ The only reason some authors like to include it is because when you take the derivative with respect to 𝑥, the 2 goes away.

Sum of Squared Error (SSE):  sum_(from i to m) (y_{b} (x^1) - y^i_{obs}) ^ 2
Total Sum of Squre (TSS):  sum_(from i to m) ( [y_{obs}] bar - y^i_{obs}) ^ 2
Coefficient of Determination:(R^2) = 1 - SSE/TSS =  (TSS − RSS)/TSS
          --   the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.


---------- Training and Test splits


---------- Cross Validation : evaluating estimator performance

training data 1
test data 1

re-select 
training data 2
validation 2

re-select
training data 3
validation 3

There is no overlap in each signle run.

complexity vs. Error
In machine learning, model complexity often refers to the number of features or terms included in a given predictive model.

overfitting: training error is low, cross validation error is high --> the variance is too high
JUST RIGHT: both training and cross validation error are low
underfitting: both training and cross validation error are high

Cross validation approaches:
k-fold cross validation: Using each of k subsamples as a test sample
leave one out cross validation： Using each observation as a test sample
stratified cross validation: k-fold cross validation with representative samples


----------  Polynomial Regression

y=b0+b1x+b2x^2+b3x^3

can also include variable interactions: y=b0+b1x+b2x2+b3x1x2



------------------------------------ Regularization Techniques

------ Bias Variance TradeOff
bias: is a tendency to miss
Variance: is a tendency to be inconsistent

Bias: being wrong
Vairance: being unstanble
Irreducible error: unavoidable randomness

------ Regularization and Model Selection 
In simple terms, regularization is tuning or selecting the preferred level of model complexity so your models are better at predicting (generalizing). 

How does Regularization acoomplished?
Regularization adds an (adjustable) regularization strength parameter directly into the cost function.

M(w) = lambda * R(w).   <-- Adjusted cost function 

M(w) model error
R(w) function of estimated parameters
lambda: regularization strengh parameter
This lambda adds a penalty proportional to the size of the estimated model parameter, or a function of the parameter.

The regularization strength parameter lambda allows us to manage the complexity tradeoff:
-- more regularization introduces a simpler model or more bias (large lambda)
-- less regularization introduction makes the model more complex and increase variance (small lambda)
If our model is overfit (variance is too high), regularization can improve the generalization error and reduce variance.

Regularization performs feature selections by shrinking the contribution of features.
L1 regularization, this is accomplished by driving some coeifficients to zero.
Feature selection can also be realized through reducing features.

Reducing the number of features can prevent overfitting (high variance).

------ Ridge Regression
Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where independent 
variables are highly correlated. 


------ LASSO Regression
Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, 
like the mean


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


----------------------------------------------------  Course 3 Suprvised Machine Learning: Classification --------------------------------------------------------------------------------------------------------
examples of models used for supervised learning: classification
Logistic regression
k-nearest neighbors
support vector machines
neural network
decision tree
random forest
boosting
ensemble models

Each of these models can be used for both regression and classifcation.



------------------------------------------------------------   Logistic Regression -----------

Curve: S-shape
logistic function: y = 1/ ( 1 + exp(- (-b0+b1x+eta) )    )
odds ratio: log(p(x) / 1 - p(x) ) = b0+b1x

---------------- Confusion Matrix, accuracy, specificity, precision and recall 

confusion matrix ( the one shows the type I type II errors)
                           Predicted Positive       Prediticed Negative
Actual positive                    TP                   FN (Type II error)  --> false negative

Actual Negative                FP (Type I error)         TN
                              false postitive
                              
                      (True Positive Rate)  Recall or sensitivity = TP/ (TP + FN). --> correctly find the postive one when it is really positive

                              Precision = TP/ (TP+FP). -->  correctly find the postive one when it is predicted postive 
                              
 ( False Positive Rate = 1- Specificity)    Specificity = TN/ (FP+TN)
                              
Accuracy = (TP+TN)/(TP+FN+TF+NF)

F1 = 2 * Precision * Recall / ( Recision + Recall)


----------------  Classification Error Metrics: ROC and Precision-Recall Curves

receiver operating characteristic (ROC) 
The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). 
Classifiers that give curves closer to the top-left corner indicate a better performance.


Precision - Recall Curve: measures trad-off betwee precision and recall

ROC curve: better for data with balanced classes
Precision-Recall curve: generally better for data with imbalanced classes


multi-class error metircs: similar to binary versions, just expand elements as a sum




---------------------------------------------- K Nearest Neighbors (KNN) for Classification ------------

I see kNN as an algorithm that comes from real life. People tend to be effected by the people around them. 
Our behaviour is guided by the friends we grew up with. Our parents also shape our personality in some ways. 
If you grow up with people who love sports, it is higly likely that you will end up loving sports. 
There are ofcourse exceptions. kNN works similarly.

What is needed to KNN?
a. correct value for K
b. How to measure closeness of neighborrs

Distance Measurement: 
Euclidean Distance d= sqrt( (x1-x2)^2 + (y1-y2)^2)
Manhattan Distance d= |x1-x2|  + |y1-y2|

Scale for distance measurement:
feature scaling

STEP 1: Define space and distance function
STEP 2: Input (UN-)known smaples
STEP 3: Find Top K nearest Samples
STEP 4: Find the majorities in the K nearest neighbor

Pros and cons of k-Nearest-Neigbors
Pros
Simple and easy to interpret
Does not make any assumption so it can be implemented in non-linear tasks.
Works well on classification with multiple classes
Works on both classification and regression tasks
Only focus on local distribution
Cons
Becomes very slow as the number of data points increases because the model needs to store all data points.
Not memory efficient
Sensitive to outliers. Outliers also have a vote!
Only focus on local distribution
Sensitive to the value of K

More exmples: recommnadations systems for video website


！！！ -------------------------------------------- Support Vector Machines  SVM----------------
In the SVM algorithm, we plot each data item as a point in n-dimensional space, with the value of each feature being the value of a particular coordinate. 
Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.
Maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane.

We have a <decision boundary /decision surface > b1x1+b2x2+b0 = 0 
we should also plot two lines which are parellel to this decision boundary, and this two lines should cross two support vectors, one
from each class, and these two lines are called the margin boundary. SVMs maximize the margin between these two parallel lines. The support vectors are the points closet to the decision line.


Linear classifier
Object: find the best beta to describe the margin boundary (hyper-plane)

Margin boundary:
b1x1+b2x2+b0=1
b1x1+b2x2+b0=-1

The distance between these two margin boundaries: dis = 2/||beta|| --> Maximize 
  => min ||Beta||^2 /2

SVM loss function:   Loss(beta) = || beta || ^ 2/ 2. HARD MARGIN SVM -> Linear classifier -> soft margin SVM
Kernel SVM. -> enable them to operate in a high-dimensiona

Margin of Separation (d): the separation between the hyperplane and the closest data point for a given weight vector w and bias.

Optimal Hyperplane (maximal margin): the particular hyperplane for which the margin of separation d is maximized.

use line linear SVMs (or logistic regression) for linear problems, 
and nonlinear kernels such as the Radial Basis Function (RBF) kernel for non-linear problems.
The RBF kernel on two samples x and x', represented as feature vectors in some input space, is defined as
K(x,x') = exp (- ||x-x'||^2  / 2* sigma).  
||x-x'||^2 can be considered as the squared Eucildean distance between two features. sigma is a free parameter.
Kernel: non-linear data can be made linear with higher dimensionality
transfermation: [x1,x2] -> [a1,a2,a3]


WORKFLOW:
problem : SVMs with RBF Kernels are very slow to train with lots of features or data.

Data collection: Consider approximate kernal map with SGD (Stochastic Gradient Descent) using Nystorem or RBF sampler.
                 Fit a linear classifier.
                 
                 
 Features                         Data                      Model Choice
 Many (~10k)                    Small (1k rows)           Simple, logistic or Linear SVM
 few(<100)                      Medium (~10k)              SVM with RBF
 few (<100)                     Many (>100k)              Add features, Logistic, Linear SVC, or Kernel Approx
 
 
 
 ---------- SVM Kernels ----------
 A kernel transforms an input data space into the required form. 
 Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space
 
 a. Linear Kernel 
 A linear kernel can be used as normal dot product any two given observations. The product between two vectors is 
 the sum of the multiplication of each pair of input values.
 K(x, xi) = sum(x * xi)
 
 b. Polynomial Kernel
 A polynomial kernel is a more generalized form of the linear kernel. 
 The polynomial kernel can distinguish curved or nonlinear input space.
 K(x,xi) = 1 + sum(x * xi)^d
 
 c. Radial Basis Function Kernel
 K(x,xi) = exp(-gamma * sum((x – xi^2))
 

 -------------------------------------------- Decision Trees ----------------
 For K nearest neighbors, training data is the model. 
 Fitting is fast - just store data
 Prediction can be slow - lots of distances to measure
 Devision boundary is flexible
 
 For logistic regression, model is just parameters.
 Fitting can be slow - must find best parameters
 Prediction is fast - calculate expected value
 Decision boundary is simple, less flexible
 
 Trees that predicts classification called the decision trees
 Node: the question 
 branch: the spliting criteria
 Leaves： where we answer the question
 
Advantages
good at interpretation and visualisation
Easy to trace back and reasoning
cons:
sensity to the predetermined maximum tree depth
Too large depth -> overfit
Too small depth -> underfit
If the dataset is large and with hign demision, it will be slow


STEP 1 : go through all the possible spliting criteria
STEP 2: select the spliting criteria with lowest entropy in the leaf node (largest entropy gain)
STEP 3: Repeat Step 1 & 2 UNTIL
       a. you reach the maximum predefined depth
       b. Leaf nodes are pure (only one class remains)
       c. a preformance metric is achieved
       
Buidling the best decision tree
-use greedy search:
find the best split at each step
- what defines the best split? One that maximizes the information gained from the split
- hwo is information gained defined? 

-classfication error equation: E(t) = 1-max[p(i|t)]. 1 - overall accuracy

< Entropy-based splitting >
 
Entropy: measures the uncertainty in a node
Strong consistency in the node -> Low entropy
Low consistency in the node - > high entropy
H(x) = -sum (i=1 to n) p(i|t) log_2 (p(i|t)); H(x): entropy, P: probability of different class in current node
after calculating the entropy for the parent and its offsprings, we can use the classification error equation above:
E(t) = 1-max[p(i|t)].     And then we can get the -> entropy change

Splitting based on entropy allows further splits to occur
can evetually reach goal of homogenous nodes

Entropy Gain: entropy ( in upper level) - sum of entropy (in current level)
G(D) = H(x) - sum (v=1 to V) |D^v|/ |D|   * H(D) ^ v. --> The larger the better


Entroy Gain concept is the ID3 (Iterative Dichotomiser 3) ALGORI: ID3 uses a top-down greedy approach to build a decision tree -> only for classfication
 
CART (classfication and regression tree) -> for both classification and regression

Trees that predict categorical results are decision trees.
 
Other Decision Tree Splitting Criteria

< Classfication Error vs Entropy >
Classification Error: 
-classification error is a flat function with maximum at center
- center represents ambiguity -- 50/50 split
- splitting metrics favor results that are furthest away from the center
Entropy:
-Entropy has the maximum but is curved
- curvature allows splitting to continue until nodes are pure

The Gini index
- in practice, Gini index often used for splitting
-function is similar to entropy - has bulge
-does not contains logarithm: G(t) = 1-sum(i=1 to n) p(i|t) ^2

<Pros and Cons>
problem: decision trees tend to overfit
Small changes in data greatly affect prediction - high variance
Solution: prune trees
- How to decide what leaves to prune? - solution: prune based on classfication error threshold

Easy to interpret and implement (if-then-else logic)
Handle any data category: binary, ordinal, continuous
No preprocessing or scalling require
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------



---------------------------  Ensemble Based Methods and Bagging
Combining models (ensemble-based methods)
bootstrap aggregation (bagging)

Decision trees tend to overfit and have high variance. Pruning helps reduce variance. Often not significant for model to generalize well.
To solve this problem:
create many different trees -> combine predictions to reduce variance
< Bagging (Bootstrap Aggregation) is used when our goal is to reduce the variance of a decision tree >
tree vote on or average result for each date point -> vote to form a single classifier
use differnt rows and see the majority class - meta classfication
Bagging = Bootstrap Aggregating

Bagging error calculations:
- same as decision trees:
  easy to interpret and implement
  Heterogeneous input data allowed, no preprocessing required
- Specific to bagging
  Less variability than decision trees
  Can grow trees in parallel
  
--------------------------- Random Forest
The random forest algorithm is actually a '' bagging''  algorithm 
For n independent trees, each with variance sigma^2, the bagged variances is: sigma^2 / n
However, bootstrap samples are correlated (rho): rho * sigma^2 + (1-rho)/n. * sigma^2
- solution: further de-correlated trees
- use random subset of features for each tree.
classification:sqrt(m),ave
regression: m/3, mode.       -- m is the number of features

----- this is the < 'Random Forest' >
A decision tree combines some decisions, whereas a random forest combines several decision trees.
A random forest is simply a collection of decision trees whose results are aggregated into one final result. 

- errors are further reduced for random forest relative to Bagging.
- Grow enough trees until error settles down.
- Addtional trees won't improve resutls

---------------------------  Bootsing and Stacking
SUPERVISED 
>>>> other meta classifiers: Boosting  -- basic idea: boosting being to built off of prior mistakes of our weak learners
     Type: Gradient Boosting, AdaBoosting
     The nature of boosting algorithms tends to produce good results in the presence of <outliers> and <rare events>.
     Boosting algorithms create trees iteratively or successively by boosting observations with high residuals from the previous tree model. 

Boosting is used to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple
models to the data and then analysing data for errors. Consecutive trees (random sample) are fit and at every step, the goal is to improve the 
accuracy from the prior tree.

Boosting means that each tree is dependent on prior trees. The algorithm learns by fitting the residual of the trees that preceded it. 

Boosting utilizess different loss functions
margin is postive for correctly classified points and negative for wrongly classified ones

>>>>>>>>  AdaBoost = Adaptive Boosting  e^(-margin) -- sentive to outliers
start with weaker learners -> focus more on the one wrong ones
Step 1: Initialize the sample weights: 
               In first step of AdaBoost each sample is associated with a weight that indicates how important it is with regards to the classification. Initially, all the samples have identical weights (1 divided by the total number of samples).
Step 2: Build a decision tree with each feature, classify the data and evaluate the result: 
               Next, for each feature, we build a decision tree with a depth of 1. Then, we use every decision tree to classify the data. Afterwards, we compare the predictions made by each tree with the actual labels in the training set.
               The feature and corresponding tree that did the best job of classifying the training samples becomes the next tree in the forest.
Step 3: Calculate the significance of the tree in the final classification    
               Once we have decided on a decision tree. We use the proceeding formula to calculate the amount of say the it has in the final classification.
               significance = 1/2 log( (1-total error) / total error ). Where the total error is the sum of the weights of the incorrectly classified samples.
Step 4: Update the sample weights so that the next decision tree will take the errors made by the preceding decision tree into account
               We look at the samples that the current tree classified <incorrectly> and increase their associated weights using the following formula.
               new sample weight = sample weight * e^significance
               Then, we look at the samples that the tree classified <correctly> and decrease their associated weights using the following formula.
               new sample weight = sample weight * e^(-significance)
               The main take away here is that the samples which the previous stump incorrectly classified should be associated with larger sample weights and the ones it classified correctly should be associated with smaller sample weights.
Step 5: Form a new dataset
               We start by making a new and empty dataset that is the same size as the original. Then, imagine a roulette table where each pocket corresponds to a sample weight. We select numbers between 0 and 1 at random. The location where each number falls determines which sample we place in the new dataset.
               Since the samples that were incorrectly classified have higher weights in relation to the others, the likelihood that the random number falls under their slice of the distribution is greater. 
               Therefore, the new dataset will have a tendency to contain multiple copies of the samples that were misclassified by the previous tree.
Step 6: Repeat steps 2 through 5 until the number of iterations equals the number specified by the hyperparameter (i.e. number of estimators)
Step 7: Use the forest of decision trees to make predictions on data outside of the training set

-- compared with random forest
They are called as ensemble learning algorithms. 
Random forest is created using a bunch of decision trees which make use of different variables or features and makes use of bagging techniques for data sample. 
In AdaBoost, the forest is created using a bunch of what is called as decision stump.  Decision stumps are nothing but decision trees with one node and two leaves. 
AdaBoost algorithm can be said to make decision using a bunch of decision stumps. Here are different posts on Random forest and AdaBoost.

AdaBoost 
pros:
The accuracy of weak classifiers can be improved by using Adaboost. 
Nowadays, Adaboost is being used to classify text and images rather than binary classification problems.
cons:
The main disadvantage of Adaboost is that it needs a quality dataset. 
Noisy data and outliers have to be avoided before adopting an Adaboost algorithm.

>>>>>>>>>>. Gradient Boosting: binomial log likelihoold loss function : log(1+e^(-margin))- more robus to outliers than AdaBoost

>>>> Stacking: combing classifiers

Labelled Data --> [Base Learners]            Meta Learner
                   Logistc Regreesion
                   SVM                      Meta Features
                   Random Forest     
models of any kind can be combined to create a stacked model.
Like bagging but not limited decision trees.
Output of base leaners can be combined via majority vote or weighted.
Additional hold-out data needed if meta learner parameters are used.
Be aware of increasing model complexity
The final prediction can be done by voting with another model


------------------------ Unbalanced classes
for unbalanced datasets, we can balane the size of the classes by either downsampling the larger class or unsampling the smalle one
upsampling (copy, duplicate the minority class)

steps for unbalanced datasets:
1. do a stratified test-train split
2. up or down sample the full dataset
3. build models

Downsamplling adds tremendous importance to the minor class, typically shooting up recall and bringing down prevision
Upsampling mitigates some of the excessive weight on the minor class. Recall is still typically higher than precision, but the gap is lesser.

How to determine to up or downsampling?
Cross-Validation works for any global model-making choice, including sampling




------------------------- self learning
Naïve Bayes algorithm is a <supervised>learning algorithm, which is based on Bayes theorem and used for solving <classificatio> problems.

It is mainly used in <text> classification that includes a high-dimensional training dataset.
Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models
that can make quick predictions.
It is a probabilistic classifier, which means it predicts on 《the basis of the probability》 of an object.
Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.




Naïve: It is called Naïve because it assumes that the occurrence of a certain feature is <independent> of the occurrence of other features. 
Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. 
Hence each feature individually contributes to identify that it is an apple without depending on each other.

Bayes: It is called Bayes because it depends on the principle of Bayes' Theorem.







--------------------------------------------------------Course 4: Unsupervised Machine Learning-----------------------------------------------------

                  Clustering                                      Dimensionality Reduction
Examples:
             K - means                                         Principal Components Analysis
             Hierarchical Agglomerative Clustering             Non-nagative Matrix Factorization
             DBSCAN                  
             Mean Shift 


unlabled data --fit--> model ---> new unlabeled data + model --predict--> map new data to model



------------------- K - means Algorithm (for clustering)

K-Means clustering is an unsupervised iterative clustering technique.
It partitions the given data set into k predefined distinct clusters.
A cluster is defined as a collection of data points exhibiting certain similarities.

Smarter initilization of K-means clusters
a. pick one point at random initially
b. pick next point with probability distance(x_i)^2/sum(i=1 to n)distance(x_i)^2


choosing the right number of clusters
Evaluating clustering performance:
- inertia: Inertia measures how well a dataset was clustered by K-Means.
 It is calculated by measuring the distance between each data point and its centroid, squaring this distance, 
 and summing these squares across one cluster.
 
sum(i=1 to n) (x_i-C_k) ^2. smaller value corresponds to tighter clusters. 
                            value senetive to number of points in clusters
                            
 A good model is one with low inertia AND a low number of clusters (K). 
  However, this is a tradeoff because as K increases, inertia decreases.
To find the optimal K for a dataset, use the <Elbow> method; find the point where the decrease in inertia begins to slow.  
  
  
                            
- Distortion: average of squared distance from each point to its cluster centriod (C_k)
1/n * sum(i=1 to n) (x_i-C_k) ^2.  smaller value corresponds to tighter clusters. 
                                   Doesn't generally increase as more points are added (relative to inertia)

Initiate multiple times, and take the model with the best score.

Introduction to K-Means Clustering
Step 1: Choose the number of clusters k. 
Step 2: Select k random points from the data as centroids.
Step 3: Assign all the points to the closest cluster centroid. 
Step 4: Recompute the centroids of newly formed clusters. 
Step 5: Repeat steps 3 and 4.

Advantages of k-means
Relatively simple to implement.
Scales to large data sets.
Guarantees convergence.
Can warm-start the positions of centroids.
Easily adapts to new examples.
Generalizes to clusters of different shapes and sizes, such as elliptical clusters.

disadvantages:
It requires to specify the number of clusters (k) in advance.
It can not handle noisy data and outliers.
It is not suitable to identify clusters with non-convex shapes.

>>>>>>>>>
Differences between KNN
KNN: only focus on samples local distribution
     K means k nearest neighbor, k points
     supervised learning method require label infromation for training

K-means: rely on all the samples distribution global distribution
         k clusters
         unsupervised learning method no need to have the label infromation for training




-------------------  Computational hurdles of clustering algorithms

Choice of distance metric is extremely important to clustering success

Euclidean Distance (L2 distance): (x11, y11) and (x22, y22) is d = √[(x2 – x1)^2 + (y2 – y1)^2].
Manhattan Distance (L1 or City Block): (X1, Y1) and (X2, Y2) is given by |X1 – X2| + |Y1 – Y2|
>>>> cosine distance: Cos(theta) = x . y / ||x|| * ||y||  .


-- Euclidean is useful for coodinate based measurements.
-- Cosine is better for data such as text where location of occurrence is less important.
-- Euclidean distance is more sensitive to curse of dimensionality.

>>>>> Jaccard Distance
Applies to sets (like word occurrence)
 J(A,B) = 1 -  the amount of value shared / the length of total unique values 
 
 
 
 
 -------------------------------------- Hierarchical Agglomerative Clustering --》 bottom up approach
 Consider which distance measure should be used
 Find closest pair, merge into a cluster 
 keeping merging closest pairs.  (pairs can be two points or two clusters. No matter what they are, merge them.)
 
 Stopping point 
 we want to cluster distances above a pre-determined threshold
 
 
 hierachical linkage types (to calculate the distance between two cluster)
 single linkage: minimium pairwise distance between clusters
 complete linkage: maximum pairwise distance between clusters
 average linkage: average pairwise distance between clusters
 ward linkage: merge based on best inertia
 --- use <dengrogram> to show the clustering process
 
 
  
 -------------------------------------- Density-Based Spatial Clustering Applications with Noise (DBSCAN) db-scan
 A true clustering algorithm: can have points dont belong to any cluster
 
 points are clustered using density of local neighborhood:
 - finds core points in high density regions and expands clusters from them
 
 Ends when all points are classfied as either belonging to a cluster or to noise
 
 Required inputs:
 - metric: function to calculate distance
 - epsilon: radius of local neighborhood -> As we mentioned, we are starting at random points and then using those points, 
                                            we're determining if other points are within a certain distance and if they are, they become part of the cluster. 
                                            Now this minimum distance between the points is going to be considered part of the same cluster if it's within a certain Epsilon range. 
                                            So that's going to be our Epsilon. How far away a point needs to be, to be considered part of that cluster.
  - N_clu: determing density threshold (for fixed eta).  ->  often seen as mean samples, which is actually the argument used for Sklearn. 
                                                          These arguments, this input will be the minimum amount of points for a particular point to be considered a core point of a cluster. 
                                                          Core points are going to be find by this N_clu arguments and they're going to be find as those points that have at least N_clu neighbors, including itself. 
                                                          So if we set N_clu equal to 3, that means that that point has at least two other neighbors that are within that Epsilon distance. 
                                                          A non-core point can still be a part of the cluster if it's in the neighborhood of that core point.  
                                                          
  Three possible labels for any point:
  core: point which has more than n_clue neighbors in their eplison-neighborhood
  density-reachable: an eplison-neighbor of a core point, and has fewer than n_clu neighbors itself
  noise: a point that has no 《core》 points in its  eplison-neighborhood
  
  Strength:
  no need to specity number of clusters (cf. k-means).  The abbreviation cf. (short for the Latin: confer/conferatur, both meaning 'compare'
  allows for noise
  can handle arbitary-shaped clusters
  
  weaknesses:
  requires two parameters (vs. one for k-means).   
  find appropriate values of eplison and n_clu can be difficult
  Does not do well with clusters of different density
  
  
   -------------------------------------- Mean Shift
   similar to k-means
   a partitioning algorithm that assigns points to nearest cluster 
   
   Centroid:
    - point of highest local density
   
   How to calculate the local density? - evaluate weighted mean around each point.
   
   steps:
   STEP 1: Pick any random point, and place the window on that data point.
   STEP 2: Calculate the mean of all the points lying inside this window.
   STEP 3: Shift the window, such that it is lying on the location of the mean.
   STEP 4: Repeat till convergence.
   STEP 5 Repeat steps 1 - 4 for all data points
   STEP 6 Data points that lead to same made are grouped into same cluster
   
   No cluster number of distance parameters needed.

Mean shift is a procedure for locating the maxima—the modes—of a density function given discrete data sampled from that function
This is an iterative method, and we start with an initial estimate x


The weighted mean of the density in the window determined by K is
m(x) = sum (xi belongs to W) xi * K(xi-x)
      --------------------------------------
          sum (xi belongs to W) K(xi-x). 
m(x) is the new mean
sum(xi belongs to W) --> sum over points in window
K  weighting (kernel) function
x: previous mean
The difference between m(x) - x is called mean shift in Fukunaga and Hostetler.  

strengths:
- model free: does not assume number or shape of clusters
- can use just one parameter: window size
- robust to outlier

weakness:
- result depends on window size
- selection of window size is not easy
- can be slow to implement, complexity proportional to mn^2 (for m iterations and n data set)

   
------------------------------------------Dimensionality Reduction----------------------------------------------------
Principal Component Analysis (PCA):
a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. 
It does so by creating new uncorrelated variables that successively maximize variance. 

Single Value Decomposition (SVD)
- SVD is a matrix factorization method normally used for PCA
- Does not require a square data set
- SVD is used by Scikit-learn for PCA   


PCA and SVD seek to find the vectors that capture the most variance.
Variance is sensitive to axis scale.
   
   
------ Moving beyong Linearity
- transformations calculated with PCA/SVD are linear
- Data can have non-linear features
- This can cause dimensionality reduction to fail.

Solutions:
- Kernal PCA: kernels can be used to perform non-linear PCA


------------------------------------------------------Course 5:  Deep Learning and Reinforcement Learning. ---------------------------------------------------------------------------------------------------------------------------

------------- Neural Networks



---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
others: Boostrap
sampling with replacement (put back,  the two sample values are independent



